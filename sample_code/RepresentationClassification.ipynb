{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"D:\\\\API-AnomalyFinder-Multivariate\\\\cases\\\\PBSF\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, f1_score, roc_curve, precision_score, recall_score\n",
    "import lightgbm as lgb\n",
    "from sklearn.svm import SVC\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_models(name, method, model_type):\n",
    "    with open(f\"../features/20210512/{method}/{name}/features.pkl\", \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    train_features = data[\"train\"][\"features\"]\n",
    "    train_labels = data[\"train\"][\"labels\"]\n",
    "    test_features = data[\"test\"][\"features\"]\n",
    "    test_labels = data[\"test\"][\"labels\"]\n",
    "    if model_type == \"lgbm\":\n",
    "        model = lgb.LGBMClassifier(num_leaves=5, n_estimators=150, learning_rate=0.05)\n",
    "    elif model_type == \"svm\":\n",
    "        model = SVC(gamma=\"auto\", probability=True)\n",
    "    model.fit(train_features, train_labels)\n",
    "    proba = model.predict_proba(test_features)\n",
    "    proba_train = model.predict_proba(train_features)\n",
    "    auc = roc_auc_score(test_labels, proba[:, 1])\n",
    "    _, _, thresholds = roc_curve(test_labels, proba[:, 1])\n",
    "    best_f1 = 0\n",
    "    for th in thresholds:\n",
    "        pred_th = np.zeros_like(proba[:, 1])\n",
    "        pred_th[proba[:, 1] > th] = 1\n",
    "        f1 = f1_score(test_labels, pred_th)\n",
    "        best_f1 = max(best_f1, f1)\n",
    "    result = {\n",
    "        \"name\": name,\n",
    "        \"best_f1\": best_f1,\n",
    "        \"auc\": auc\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exams = {\n",
    "    \"patient_id_1\": {\n",
    "        \"method\": \"moco\",\n",
    "        \"model_type\": \"svm\"\n",
    "    },\n",
    "    \"patient_id_2\": {\n",
    "        \"method\": \"contrastive\",\n",
    "        \"model_type\": \"svm\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for name, config in exams.items():\n",
    "    results.append(\n",
    "        validate_models(name, config[\"method\"], config[\"model_type\"])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in results:\n",
    "    print(f\"{x['name']}\\tF1 score: {x['best_f1']:.2%}\\tAUC score: {x['auc']:.3}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
